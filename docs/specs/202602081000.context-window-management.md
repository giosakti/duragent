# Context Window Management

**Status:** Implemented
**Priority:** P0

## The Problem

Every LLM call sends the **entire** conversation history from the beginning of the session. There is no truncation, windowing, or summarization.

For gateway sessions (Telegram, Discord) that are long-lived and reused indefinitely, this means:
1. Token usage grows linearly with conversation length — every message costs more than the last
2. Eventually the conversation exceeds the provider's context window limit → hard error, no graceful degradation
3. The session becomes permanently unusable (every subsequent call will also fail)
4. `state.yaml` and actor memory grow unbounded

The `max_input_tokens` field already exists in `ModelConfig` but is never used.

### Current Flow

**Conversation:**
```
handle.get_messages()           → returns ALL messages (checkpointed + pending)
ContextBuilder::with_messages() → passes them through unchanged
StructuredContext::render()     → [system, ...all_history] → ChatRequest
provider.chat()                 → sends everything to LLM
```

This applies to all three call sites: HTTP handler, gateway handler, and scheduler.

**Agentic loop** (`agentic_loop.rs`) has a more acute problem — the local `messages` vec grows every iteration with no truncation between iterations:
```
initial_messages = ContextBuilder.render().messages    → full conversation history
loop {
    request = ChatRequest::with_tools(messages.clone())  → sends EVERYTHING
    response = provider.chat_stream(request)
    messages.push(assistant_msg_with_tool_calls)         → grows
    for tool_call in tool_calls {
        messages.push(tool_result_msg)                   → grows (tool output can be huge)
    }
}
```

A 10-iteration loop with 2 tool calls per iteration can easily consume 100k+ tokens of tool results alone — on top of the conversation history. This vec is a local variable, completely disconnected from the actor and `ContextBuilder`.

**Storage:** The `SessionActor` maintains `checkpointed_messages` (stable, in `state.yaml`) and `pending_messages` (recent, replayed from `events.jsonl`). Both grow without bound.

## Solution: Three Layers of Truncation

| Layer | What it bounds | When it runs | Growth pattern |
|-------|---------------|--------------|----------------|
| **1. Actor-level window** | In-memory messages, `state.yaml` snapshot | On every message add | Cross-turn: conversation grows over hours/days |
| **2. Render-time budget** | Messages in initial `ChatRequest` | On every LLM call | Cross-turn: precise fit to context window |
| **3. Agentic loop budget** | Messages between tool iterations | Between loop iterations | Intra-turn: tool calls accumulate within one turn |

### Token Counting

Use a simple heuristic: `token_count ≈ byte_length / 4`. Conservative for English text, avoids model-specific tokenizer dependency. The 10% safety margin covers inaccuracy.

### Model-Level Configuration

Use the existing `max_input_tokens` field in agent spec's `ModelConfig`:

```yaml
model:
  provider: anthropic
  name: claude-sonnet-4-20250514
  max_input_tokens: 150000   # optional, see defaults below
  max_output_tokens: 8192
```

When not set, defaults are derived from the model name:

```rust
fn default_context_window(model_name: &str) -> u32 {
    // More specific version patterns checked before family catch-alls.
    // Context windows sourced from OpenRouter model pages (Feb 2026).
    let name = model_name.to_lowercase();
    match name.as_str() {
        s if s.contains("claude") => 200_000,
        s if s.contains("gpt-5") => 400_000,
        s if s.contains("gpt-4.1") => 1_000_000,
        s if s.contains("gpt-4o") => 128_000,
        s if s.contains("gpt-4-turbo") => 128_000,
        s if s.contains("gpt-4") => 128_000,
        s if s.contains("gemini") => 1_000_000,
        s if s.contains("grok-4") => 2_000_000,
        s if s.contains("grok") => 131_072,
        s if s.contains("deepseek-v3") || s.contains("deepseek-chat-v3") => 163_840,
        s if s.contains("deepseek") => 128_000,
        s if s.contains("qwen3") => 131_072,
        s if s.contains("qwen") => 128_000,
        s if s.contains("llama-4") => 327_680,
        s if s.contains("llama") => 128_000,
        s if s.contains("mistral-large") => 262_144,
        s if s.contains("mistral") || s.contains("mixtral") => 128_000,
        _ => 128_000,
    }
}
```

No config needed for most users — works out of the box. Set `max_input_tokens` to cap usage (cost control) or raise it for custom models.

## Layer 1: Actor-Level Window

A message-count bound on the actor, **derived internally** from `max_input_tokens`:

```rust
fn actor_message_limit(max_input_tokens: u32) -> usize {
    // Assume ~200 tokens per message on average, 2x headroom for Layer 2 to trim precisely
    let derived = (max_input_tokens as usize / 200) * 2;
    derived.clamp(100, 2000)
}

fn maybe_trim_history(&mut self) {
    let total = self.checkpointed_messages.len() + self.pending_messages.len();
    if total > self.actor_message_limit && !self.checkpointed_messages.is_empty() {
        let excess = total - self.actor_message_limit;
        let drain_count = excess.min(self.checkpointed_messages.len());
        self.checkpointed_messages.drain(..drain_count);
    }
}
```

**Where:** In `SessionActor::add_user_message()` and `add_assistant_message()`, after `maybe_roll_checkpoint()`.

Not a user-facing knob. The 2x headroom lets the actor hold more messages than will fit in the context window — Layer 2 handles the precise trim at render time.

A `DEFAULT_ACTOR_MESSAGE_LIMIT` of 1000 is used when the agent spec isn't available (e.g., during session recovery or in the scheduler).

## Layer 2: Render-Time Budget

Token-aware truncation when building the `ChatRequest`. Two caps apply — whichever is more restrictive wins:

```
effective_limit = max_input_tokens OR default_context_window(model_name)

available = effective_limit
          - estimate_tokens(system_prompt + directives)
          - estimate_tokens(tool_definitions)
          - max_output_tokens  (reserve for response)
          - safety_margin      (10% of effective_limit)

history_budget = min(available, max_history_tokens) if max_history_tokens > 0
               = available                          if max_history_tokens == 0

Fill `history_budget` with messages from newest to oldest.
If any messages were omitted, inject a truncation notice.
```

**Truncation notice** — injected as a `Role::System` message when messages are omitted so the model knows it's missing context:
```
[conversation truncated — 47 older messages omitted]
```

`max_history_tokens` (default: 20000) lets users reserve context budget for tool work. With a 200k context window and `max_history_tokens: 20000`, the agent sends at most ~20k tokens of history — leaving the rest for tool calls and results during the agentic loop. Set `0` to disable the history cap.

**Where:** `StructuredContext::render_with_budget()` — all three call sites (HTTP, gateway, scheduler) construct a `TokenBudget` and pass it at render time. The `ContextBuilder` remains budget-unaware; it builds the `StructuredContext`, and the caller invokes `render_with_budget()` on the result.

## Layer 3: Agentic Loop Budget

Context management **within a single agentic turn**, applied to the local `messages` vec in `run_agentic_loop()`. Three sub-layers, graduated from least to most aggressive. Most turns only need 3a.

The token budget for Layer 3c is calculated once before the loop starts: `max_input_tokens - max_output_tokens - safety_margin(10%)`. This is a simplified formula compared to Layer 2 — it does not subtract system prompt or tool definition tokens, relying on the safety margin to cover them.

### Sub-layer 3a: Tool Result Truncation

Cap individual tool results at `max_tool_result_tokens` (default: 8000, must be > 0). Applied immediately when the tool returns, before pushing to `messages`.

**Truncation strategies** (`tool_result_truncation`):

| Strategy | Keeps | Use case |
|----------|-------|----------|
| `head` (default) | Beginning of output | Command output, search results, file headers |
| `tail` | End of output | Log files, build output |
| `both` | Beginning + end (50/50 split) | Large files where both header and footer matter |

**Truncation indicators** (appended/prepended to the truncated content):

| Strategy | Indicator format |
|----------|-----------------|
| `head` | `[truncated: kept first ~8000 of ~52000 tokens (head)]` |
| `tail` | `[truncated: kept last ~8000 of ~52000 tokens (tail)]` |
| `both` | `[truncated: kept first+last ~8000 of ~52000 tokens (both)]` (between the two halves) |

### Sub-layer 3b: Observation Masking

Always-on masking that keeps the **first N** and **last M** tool results visible, masking everything in between. Tool calls remain visible (the agent knows what it did), but middle results are replaced:

```
[result masked — ~12000 tokens removed]
```

- First results establish context (file structure, codebase layout)
- Last results are the active work the agent is building on
- Middle results are intermediate exploration — expendable
- If total results <= `keep_first + keep_last`, nothing gets masked
- Setting both to `0` disables masking

Applied between iterations, before building the next `ChatRequest`.

### Sub-layer 3c: Iteration Group Dropping (Last Resort)

If masking isn't enough, drop the oldest complete iteration groups (`assistant_with_tool_calls + tool_results`). Drop oldest to newest, always preserve conversation history and the most recent iteration. Never break a tool call / tool result pair.

Rarely triggers in practice — 3a and 3b handle the vast majority of cases.

### Graduated Flow

```
Tool returns 52,000 token result
    ↓
3a: Truncate to 8,000 tokens                    ← handles most cases
    ↓
Next iteration: more than keep_first + keep_last results?
    ↓ yes
3b: Mask middle tool results with placeholder    ← handles heavy tool use
    ↓
Still over budget?
    ↓ yes (rare)
3c: Drop oldest iteration groups entirely        ← last resort
```

### Message Structure Example

After 8 iterations with `keep_first: 2, keep_last: 3`, 3a+3b applied:

```
[system]                                          ← always preserved
[user_1] [assistant_1] ... [user_N]              ← conversation history (Layer 2)
[assistant_tool_call_1] [result: 8k truncated]   ← iteration 1: visible (keep_first)
[assistant_tool_call_2] [result: 8k truncated]   ← iteration 2: visible (keep_first)
[assistant_tool_call_3] [result: masked]          ← iteration 3: masked (middle)
[assistant_tool_call_4] [result: masked]          ← iteration 4: masked (middle)
[assistant_tool_call_5] [result: masked]          ← iteration 5: masked (middle)
[assistant_tool_call_6] [result: 8k truncated]   ← iteration 6: visible (keep_last)
[assistant_tool_call_7] [result: 8k truncated]   ← iteration 7: visible (keep_last)
[assistant_tool_call_8] [result: 8k truncated]   ← iteration 8: visible (keep_last)
```

## Configuration

```yaml
session:
  on_disconnect: pause
  max_tool_iterations: 25
  context:
    max_history_tokens: 20000       # Layer 2: cap history tokens to reserve budget for tool work (0 = no cap)
    max_tool_result_tokens: 8000    # Layer 3a: per-result token cap
    tool_result_truncation: head    # Layer 3a: head, tail, or both
    tool_result_keep_first: 2       # Layer 3b: first N tool results kept visible
    tool_result_keep_last: 5        # Layer 3b: last M tool results kept visible
```

All fields have sensible defaults — the entire `context` block can be omitted:

| Field | Type | Default | Layer | Description |
|-------|------|---------|-------|-------------|
| `max_history_tokens` | int | `20000` | 2 | Max tokens for conversation history. Reserves remaining budget for tool work. `0` = no cap. |
| `max_tool_result_tokens` | int | `8000` | 3a | Max tokens per tool result. Must be > 0. |
| `tool_result_truncation` | string | `head` | 3a | How to truncate: `head`, `tail`, or `both` (50/50 split). |
| `tool_result_keep_first` | int | `2` | 3b | First N tool results kept visible. |
| `tool_result_keep_last` | int | `5` | 3b | Last M tool results kept visible. Both `0` = no masking. |

Layer 1 is derived internally from `max_input_tokens`. Layer 3c has no knob — it's a safety net.

## Acceptance Criteria

### Conversation-Level (Layers 1 & 2)
- [x] Messages are truncated to fit within the model's context window before each LLM call
- [x] System prompt, tools, and output token reserve are accounted for in the budget
- [x] Most recent messages preserved (oldest dropped first)
- [x] Truncation notice injected when messages are omitted
- [x] Works for all three call sites (HTTP, gateway, scheduler)
- [x] `max_input_tokens` respected when set; sensible defaults per model name when not
- [x] Actor memory and `state.yaml` bounded by derived limit
- [x] Full conversation history remains available in `events.jsonl` for audit

### Agentic Loop (Layer 3)
- [x] Individual tool results truncated to `max_tool_result_tokens` (3a)
- [x] All three truncation strategies work: `head`, `tail`, `both`
- [x] Truncated results include indicator with original vs truncated size and strategy
- [x] `max_tool_result_tokens` must be > 0 (validated at config load)
- [x] Tool results outside first/last window are masked (3b)
- [x] `tool_result_keep_first` and `tool_result_keep_last` control visible windows
- [x] Results <= `keep_first + keep_last` means nothing gets masked; both `0` disables masking
- [x] Masked results preserve tool call (arguments visible), replace content
- [x] Oldest iteration groups dropped as last resort (3c); conversation history always preserved
- [x] Agent with many tool iterations no longer hits context limit errors mid-loop

### Configuration
- [x] All `session.context` fields have working defaults (entire block is optional)
- [x] Actor-level window derived from `max_input_tokens` (not user-configurable)
- [x] `max_history_tokens` caps history token budget (default `20000`, `0` = no cap)
- [x] `tool_result_keep_first` and `tool_result_keep_last` control observation masking (both `0` = disabled)
- [x] Duragent Format spec documents the `context` block

## Out of Scope

- Conversation summarization (compressing dropped messages into a summary)
- Exact tokenizer integration (tiktoken, Anthropic tokenizer)
- Per-message importance scoring
- Caching/reusing KV cache across calls (provider-side concern)
- Peek tools for accessing truncated content (future: `peek_head(id)`, `peek_grep(id, pattern)`)
- Per-tool truncation overrides (e.g., different cap for `bash` vs `web_search`)

## Technical Notes

- The heuristic `bytes / 4` overestimates for ASCII, underestimates for CJK. The 10% safety margin covers this.
- Tool definitions can be large — must be included in budget calculation.
- `events.jsonl` with compaction (see session-lifecycle spec) keeps events after `checkpoint_seq`. Full history reconstruction requires reading events before compaction — if full history APIs are needed later, compaction should be made optional or events archived before pruning.
- The `context` block is nested under `session` rather than flat because it groups related concerns. The Duragent Format spec should document this as a new section under `spec.session`.
